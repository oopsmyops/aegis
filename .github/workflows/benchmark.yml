name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-benchmark-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark memory-profiler

    - name: Create benchmark test
      run: |
        mkdir -p benchmarks
        cat > benchmarks/test_performance.py << 'EOF'
        """Performance benchmarks for AEGIS CLI components."""
        
        import pytest
        import time
        from unittest.mock import Mock, patch
        
        # Import AEGIS components
        from aegis.discovery.discovery import ClusterDiscovery
        from aegis.questionnaire.questionnaire_runner import QuestionnaireRunner
        from aegis.catalog.catalog_manager import PolicyCatalogManager
        from aegis.ai.ai_policy_selector import AIPolicySelector
        
        
        class TestDiscoveryPerformance:
            """Benchmark cluster discovery operations."""
            
            @pytest.mark.benchmark(group="discovery")
            def test_cluster_info_collection(self, benchmark):
                """Benchmark cluster information collection."""
                with patch('kubernetes.config.load_kube_config'):
                    with patch('kubernetes.client.CoreV1Api') as mock_api:
                        mock_api.return_value.list_node.return_value.items = []
                        mock_api.return_value.list_namespace.return_value.items = []
                        
                        discovery = ClusterDiscovery()
                        result = benchmark(discovery.collect_basic_info)
                        assert result is not None
        
        
        class TestQuestionnairePerformance:
            """Benchmark questionnaire operations."""
            
            @pytest.mark.benchmark(group="questionnaire")
            def test_question_processing(self, benchmark):
                """Benchmark question processing speed."""
                runner = QuestionnaireRunner()
                
                def process_questions():
                    # Simulate processing all questions
                    questions = runner.question_bank.get_all_questions()
                    return len(questions)
                
                result = benchmark(process_questions)
                assert result > 0
        
        
        class TestCatalogPerformance:
            """Benchmark policy catalog operations."""
            
            @pytest.mark.benchmark(group="catalog")
            def test_policy_indexing(self, benchmark):
                """Benchmark policy indexing performance."""
                with patch('os.path.exists', return_value=True):
                    with patch('os.listdir', return_value=['policy1.yaml', 'policy2.yaml']):
                        with patch('builtins.open', create=True) as mock_open:
                            mock_open.return_value.__enter__.return_value.read.return_value = """
                            apiVersion: kyverno.io/v1
                            kind: ClusterPolicy
                            metadata:
                              name: test-policy
                            """
                            
                            manager = PolicyCatalogManager()
                            result = benchmark(manager.build_lightweight_index)
                            assert result is not None
        
        
        class TestAIPerformance:
            """Benchmark AI operations (mocked)."""
            
            @pytest.mark.benchmark(group="ai")
            def test_policy_selection_logic(self, benchmark):
                """Benchmark policy selection logic (without AI calls)."""
                selector = AIPolicySelector()
                
                # Mock policy data
                mock_policies = [
                    {"name": f"policy-{i}", "category": "security", "tags": ["test"]}
                    for i in range(100)
                ]
                
                def selection_logic():
                    # Simulate policy filtering logic
                    filtered = [p for p in mock_policies if "security" in p["category"]]
                    return len(filtered)
                
                result = benchmark(selection_logic)
                assert result > 0
        
        
        class TestMemoryUsage:
            """Memory usage benchmarks."""
            
            def test_memory_usage_discovery(self):
                """Test memory usage during discovery."""
                import tracemalloc
                
                tracemalloc.start()
                
                with patch('kubernetes.config.load_kube_config'):
                    with patch('kubernetes.client.CoreV1Api') as mock_api:
                        mock_api.return_value.list_node.return_value.items = []
                        mock_api.return_value.list_namespace.return_value.items = []
                        
                        discovery = ClusterDiscovery()
                        discovery.collect_basic_info()
                
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                
                # Memory usage should be reasonable (less than 50MB peak)
                assert peak < 50 * 1024 * 1024, f"Peak memory usage too high: {peak / 1024 / 1024:.2f} MB"
                print(f"Memory usage - Current: {current / 1024 / 1024:.2f} MB, Peak: {peak / 1024 / 1024:.2f} MB")
        EOF

    - name: Run benchmarks
      run: |
        python -m pytest benchmarks/ -v --benchmark-only --benchmark-json=benchmark-results.json

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name != 'pull_request'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: false

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark-results.json
        retention-days: 30

    - name: Performance summary
      run: |
        echo "## ðŸ“Š Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f benchmark-results.json ]; then
          echo "Benchmark results have been generated and stored." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Key Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- All benchmarks completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "- Memory usage tests passed" >> $GITHUB_STEP_SUMMARY
          echo "- Performance data available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Benchmark results not generated" >> $GITHUB_STEP_SUMMARY
        fi

  binary-size-check:
    name: Binary Size Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pyinstaller

    - name: Build binary for size analysis
      run: |
        pyinstaller --onefile \
          --name aegis-size-test \
          --add-data "aegis-config.yaml:." \
          --hidden-import=aegis.cli.main \
          --hidden-import=aegis.discovery.discovery \
          --hidden-import=aegis.questionnaire.questionnaire_runner \
          --hidden-import=aegis.catalog.catalog_manager \
          --hidden-import=aegis.ai.ai_policy_selector \
          --hidden-import=boto3 \
          --hidden-import=botocore \
          --hidden-import=kubernetes \
          --hidden-import=yaml \
          --hidden-import=click \
          --console \
          aegis/cli/main.py

    - name: Analyze binary size
      run: |
        BINARY_PATH="dist/aegis-size-test"
        BINARY_SIZE=$(stat -c%s "$BINARY_PATH")
        BINARY_SIZE_MB=$(echo "scale=2; $BINARY_SIZE / 1024 / 1024" | bc)
        
        echo "## ðŸ“¦ Binary Size Analysis" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Binary Size**: ${BINARY_SIZE_MB} MB (${BINARY_SIZE} bytes)" >> $GITHUB_STEP_SUMMARY
        echo "- **Build Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
        echo "- **Platform**: Linux x64" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Size thresholds
        MAX_SIZE_MB=100
        WARN_SIZE_MB=80
        
        if (( $(echo "$BINARY_SIZE_MB > $MAX_SIZE_MB" | bc -l) )); then
          echo "âŒ **Warning**: Binary size exceeds ${MAX_SIZE_MB} MB threshold" >> $GITHUB_STEP_SUMMARY
          echo "Consider optimizing dependencies or using UPX compression" >> $GITHUB_STEP_SUMMARY
        elif (( $(echo "$BINARY_SIZE_MB > $WARN_SIZE_MB" | bc -l) )); then
          echo "âš ï¸ **Notice**: Binary size approaching ${MAX_SIZE_MB} MB limit" >> $GITHUB_STEP_SUMMARY
        else
          echo "âœ… Binary size is within acceptable limits" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Store size for comparison
        echo "$BINARY_SIZE" > binary-size.txt

    - name: Upload size data
      uses: actions/upload-artifact@v4
      with:
        name: binary-size-data
        path: binary-size.txt
        retention-days: 90